---
title: "Intoxication Generators, try 2"
author: "Julin Maloof"
date: "7/21/2021"
output: html_document
---

```{r}
library(tidyverse)
library(keras)
use_condaenv("r-reticulate")
```

Data was originally collected from 19 participants, but the TAC readings of 6 participants were deemed unusable by SCRAM [1]. The data included is from the remaining 13 participants.

Accelerometer data was collected from smartphones at a sampling rate of 40Hz (file: all_accelerometer_data_pids_13.csv). The file contains 5 columns: a timestamp, a participant ID, and a sample from each axis of the accelerometer. Data was collected from a mix of 11 iPhones and 2 Android phones as noted in phone_types.csv. TAC data was collected using SCRAM [2] ankle bracelets and was collected at 30 minute intervals. The raw TAC readings are in the raw_tac directory. TAC readings which are more readily usable for processing are in clean_tac directory and have two columns: a timestamp and TAC reading. The cleaned TAC readings: (1) were processed with a zero-phase low-pass filter to smooth noise without shifting phase; (2) were shifted backwards by 45 minutes so the labels more closely match the true intoxication of the participant (since alcohol takes about 45 minutes to exit through the skin.) Please see the above referenced study for more details on how the data was processed (http://ceur-ws.org/Vol-2429/paper6.pdf).

1 - https://www.scramsystems.com/
2 - J. Robert Zettl. The determination of blood alcohol concentration by transdermal measurement. https://www.scramsystems.com/images/uploads/general/research/the-determination-of-blood-alcohol-concentrationby-transdermal-measurement.pdf, 2002.

5. Number of Instances:
Accelerometer readings: 14,057,567
TAC readings: 715
Participants: 13

6. Number of Attributes:
- Time series: 3 axes of accelerometer data (columns x, y, z in all_accelerometer_data_pids_13.csv)
- Static: 1 phone-type feature (in phone_types.csv)
- Target: 1 time series of TAC for each of the 13 participants (in clean_tac directory).

7. For Each Attribute:
(Main)
all_accelerometer_data_pids_13.csv:
time: integer, unix timestamp, milliseconds
pid: symbolic, 13 categories listed in pids.txt 
x: continuous, time-series
y: continuous, time-series
z: continuous, time-series
clean_tac/*.csv:
timestamp: integer, unix timestamp, seconds
TAC_Reading: continuous, time-series
phone_type.csv:
pid: symbolic, 13 categories listed in pids.txt 
phonetype: symbolic, 2 categories (iPhone, Android)

(Other)
raw/*.xlsx:
TAC Level: continuous, time-series
IR Voltage: continuous, time-series
Temperature: continuous, time-series
Time: datetime
Date: datetime

8. Missing Attribute Values:
None

9. Target Distribution:
TAC is measured in g/dl where 0.08 is the legal limit for intoxication while driving
Mean TAC: 0.065 +/- 0.182
Max TAC: 0.443
TAC Inner Quartiles: 0.002, 0.029, 0.092
Mean Time-to-last-drink: 16.1 +/- 6.9 hrs

## Read the data

### TAC
```{r}
tac_dir <- "intox_data/clean_tac/"
tac <- tibble(file=dir(path=tac_dir, pattern="csv$"))
tac <- tac %>% mutate(data=map(file, ~ read_csv(file.path(tac_dir, .x)))) %>% 
  unnest(data) %>%
  mutate(pid=str_remove(file, "_.*"), # get units to match accelerometer data
         time_30m=as.integer(timestamp/60/30)) %>% # for combining.  as.integer required for matching.
  select(time_30m, timestamp, pid, TAC_Reading) %>%
  arrange(pid, timestamp)
tac
```

Note that we could convert the timestamp if we wanted to, but I don't think we do
```{r}
as.POSIXlt(tac$timestamp[1:10], origin="1970-01-01")
```

### acceleromator
```{r}
accel <- read_csv("intox_data/all_accelerometer_data_pids_13.csv") %>%
  arrange(pid, time) %>%
  filter(time > 0) # git rid of a couple of 0 readings
```

```{r}
accel <- accel %>%  mutate(time_30m = as.integer(round(time/1000/60/30)))
accel.u <- accel %>% filter(!duplicated(time_30m))
accel.u
```
## compare times

```{r}
tactime <- tac %>% group_by(pid) %>%
  summarize(first=as.POSIXlt(min(timestamp), origin="1970-01-01"),
            last=as.POSIXlt(max(timestamp), origin="1970-01-01")) %>%
  mutate(source="tac")

acceltime <- accel %>% group_by(pid) %>%
  summarize(first=as.POSIXlt(min(time)/1000, origin="1970-01-01"),
            last=as.POSIXlt(max(time)/1000, origin="1970-01-01")) %>%
  mutate(source="acc")

tatime <- rbind(tactime, acceltime)
 tatime %>% arrange(pid, source)
```
```{r}
tatime %>%
  ggplot(aes(xmin=first,xmax=last,y=pid, color=source)) +
  geom_linerange(position = position_dodge(width=.6), lwd=3) +
  theme(axis.text.x = element_text(angle=90))
```

## plot actual measurements

```{r}
tac %>% mutate(realtime=as.POSIXct(timestamp, origin="1970-01-01")) %>%
  ggplot(aes(x=realtime, y=pid)) +
  geom_point(shape=3)
```

```{r}
tac %>% mutate(ET=as.POSIXct(timestamp, origin="1970-01-01", tz = "US/Eastern")) %>%
  ggplot(aes(x=ET, y=TAC_Reading)) +
  geom_line() +
  facet_wrap(~pid, ncol=3) +
  theme(axis.text.x = element_text(angle=45, hjust = 1))
```

## combine and wrangle

```{r}
combined <- inner_join(accel, tac) %>%
  mutate(intox=(TAC_Reading > 0.08) * 1L)

head(combined)
```

## Generator:

```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 32, step = 1) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay # - 1 why "-1"?  removed it
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                3)) # 3 for x, y, z
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      my_row <- rows[j]
      
      indices <- seq(my_row - lookback, my_row, 
                     length.out = dim(samples)[[2]])
      
      # make sure all from same person
      discord <- sum(data$pid[indices] != data$pid[my_row])
      if (discord > 0) indices <- indices + discord
      
      samples[j,,] <- as.matrix(data[round(indices),c("x", "y", "z")])
      targets[[j]] <- as.matrix(data[my_row + delay, "intox"])
    }            
    
    list(samples, targets)
  }
}
```

```{r}
lookback <- 24000 # 600 seconds
step <- 2 # 20 observations per second
delay <- 0
batch_size <- 128

train_gen <- generator(
  data = combined,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = {which(combined$pid=="JR8022") %>% max()},
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

val_gen = generator(
  data = combined,
  lookback = lookback,
  delay = delay,
  min_index = {which(combined$pid=="JR8022") %>% max() + 1},
  max_index = {which(combined$pid=="MJ8002") %>% max()},
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

test_gen <- generator(
  data = combined,
  lookback = lookback,
  delay = delay,
  min_index = {which(combined$pid=="MJ8002") %>% max() + 1},
  max_index = nrow(combined),
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)

# Unrealistic to look at the whole validation or test sets; look at 10000
val_steps <- (1000) / batch_size

# This is how many steps to draw from `test_gen`
# in order to see the whole test set:
test_steps <- (10000) / batch_size
```

## Does this work?

## Simple dense network

```{r, echo=TRUE}

set.seed(123)

model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, 3)) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 50,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

Let's display the loss curves for validation and training:

```{r}
plot(history)
```

## simple RNN


```{r echo=TRUE, results='hide'}
model <- keras_model_sequential() %>% 
  layer_gru(units = 32, input_shape = list(NULL, 3)) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 50,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```